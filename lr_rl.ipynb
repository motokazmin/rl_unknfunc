{"cells":[{"cell_type":"markdown","metadata":{"id":"2TO7phA-kqx6"},"source":["**Пробуем настроить custom learning rate процедуру с помощью Reinforcement Learning**"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","executionInfo":{"elapsed":998,"status":"ok","timestamp":1645221570550,"user":{"displayName":"Roman Kazmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09610597500933245699"},"user_tz":-180},"id":"h7ETTcsQfs6e"},"outputs":[],"source":["#@title Import { form-width: \"10%\" }\n","import wrappertask as wt"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":964,"status":"ok","timestamp":1645221571511,"user":{"displayName":"Roman Kazmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09610597500933245699"},"user_tz":-180},"id":"H54i8jrQkjZG"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#@title Classes { vertical-output: true, form-width: \"10%\" }\n","import numpy as np\n","import torch\n","import datetime\n","import random, copy\n","from collections import deque\n","from pathlib import Path\n","from gym import Env\n","from gym.spaces import Discrete, Box\n","\n","class UnknownFuncEnv(Env):\n","  def __init__(self, epochs, debug = False, use_const = False, lr_task = 0.002):\n","    self.epochs = epochs\n","    self.debug = debug\n","    self.use_const = use_const\n","    self.lr_task = lr_task\n","    self.lr = [0.0000001, 0.0000005, 0.000001, 0.000005, 0.0001, 0.0005, 0.001, 0.002, 0.004, 0.01, 0.1] #!\n","    self.action_space = Discrete(len(self.lr))\n","    self.observation_space = Box(low=np.float32(np.array([0, 0])),\n","                                 high=np.array([np.finfo(np.float32).max, np.finfo(np.float32).max]))\n","\n","  def step(self, action):\n","    rmse, std = self.task.train_epoch()\n","    self.state = np.array([rmse, std])\n","    done = self.task.done()\n","\n","    if self.use_const == False:\n","      self.task.set_scheduler_lr(self.lr[action])\n","    else:\n","      self.task.set_scheduler_lr(0.002)\n","\n","    reward = 0\n","    if self.prev_rmse != -1:\n","      if rmse < self.prev_rmse:\n","        reward = 1\n","      else:\n","        reward = -1\n","\n","    self.prev_rmse = rmse\n","    \n","    self.total_reward += reward\n","    info = {'episode reward': self.total_reward}\n","\n","    return self.state, reward, done, info\n","\n","  def reset(self):\n","    self.task = wt.TaskWrapper(debug = self.debug, epochs = self.epochs, lr=self.lr_task)\n","    self.state = np.ndarray([2])\n","    self.total_reward = 0\n","    self.prev_rmse = -1\n","\n","    return self.state\n","\n","class UnknownFuncRLNet(torch.nn.Module):\n","  def __init__(self, input_dim, output_dim):\n","    super().__init__()\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","\n","    self.online = torch.nn.Sequential( #!\n","      torch.nn.Linear(input_dim, 128),\n","      torch.nn.ReLU(),\n","      torch.nn.Linear(128, 64),\n","      torch.nn.ReLU(),\n","      torch.nn.Linear(64, output_dim)\n","    )\n","    self.target = copy.deepcopy(self.online)\n","\n","    # Q_target parameters are frozen.\n","    for p in self.target.parameters():\n","      p.requires_grad = False\n","\n","  def forward(self, input, model):\n","    if model == \"online\":\n","        return self.online(input.float())\n","    elif model == \"target\":\n","        return self.target(input.float())\n","\n","class UnknownFuncAgent():\n","  def __init__(self, state_dim, action_dim, save_dir):\n","    self.state_dim = state_dim\n","    self.action_dim = action_dim\n","    self.save_dir = save_dir\n","\n","    self.use_cuda = torch.cuda.is_available()\n","\n","    self.net = UnknownFuncRLNet(np.shape(self.state_dim)[0], self.action_dim).float()\n","    if self.use_cuda:\n","        self.net = self.net.to(device=\"cuda\")\n","\n","    self.exploration_rate = 1\n","    self.exploration_rate_decay =  0.9 #!\n","    self.exploration_rate_min = 0.1\n","    self.curr_step = 0\n","\n","    self.save_every = 2000\n","\n","    self.memory = deque(maxlen=100000) #!\n","    self.batch_size = 32\n","\n","    self.gamma = 0.9\n","\n","    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n","    self.loss_fn = torch.nn.SmoothL1Loss()\n","\n","    self.burnin = 1e4\n","    self.learn_every = 3\n","    self.sync_every = 1e4 \n","    \n","  def act(self, state):\n","    if np.random.rand() < self.exploration_rate:\n","      action_idx = np.random.randint(self.action_dim)\n","    else:\n","      state = state.__array__()\n","      if self.use_cuda:\n","          state = torch.tensor(state).cuda()\n","      else:\n","          state = torch.tensor(state)\n","      state = state.unsqueeze(0)\n","      action_values = self.net(state, model=\"online\")\n","      action_idx = torch.argmax(action_values, axis=1).item()\n","\n","    self.exploration_rate *= self.exploration_rate_decay\n","    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","\n","    self.curr_step += 1\n","    return action_idx\n","\n","  def cache(self, state, next_state, action, reward, done):\n","    state = state.__array__()\n","    next_state = next_state.__array__()\n","\n","    if self.use_cuda:\n","      state = torch.tensor(state).cuda()\n","      next_state = torch.tensor(next_state).cuda()\n","      action = torch.tensor([action]).cuda()\n","      reward = torch.tensor([reward]).cuda()\n","      done = torch.tensor([done]).cuda()\n","    else:\n","      state = torch.tensor(state)\n","      next_state = torch.tensor(next_state)\n","      action = torch.tensor([action])\n","      reward = torch.tensor([reward])\n","      done = torch.tensor([done])\n","\n","    self.memory.append((state, next_state, action, reward, done,))\n","\n","  def recall(self):\n","    batch = random.sample(self.memory, self.batch_size)\n","    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n","\n","  def td_estimate(self, state, action):\n","    current_Q = self.net(state, model=\"online\")[\n","        np.arange(0, self.batch_size), action\n","    ]  # Q_online(s,a)\n","    return current_Q\n","\n","  @torch.no_grad()\n","  def td_target(self, reward, next_state, done):\n","    next_state_Q = self.net(next_state, model=\"online\")\n","    best_action = torch.argmax(next_state_Q, axis=1)\n","    next_Q = self.net(next_state, model=\"target\")[\n","        np.arange(0, self.batch_size), best_action\n","    ]\n","    return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n","\n","  def update_Q_online(self, td_estimate, td_target):\n","    loss = self.loss_fn(td_estimate, td_target)\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()\n","    return loss.item()\n","\n","  def sync_Q_target(self):\n","    self.net.target.load_state_dict(self.net.online.state_dict())\n","\n","  def save(self):\n","    save_path = (\n","      self.save_dir / f\"unknown_func_net_{int(self.curr_step // self.save_every)}.chkpt\"\n","    )\n","    torch.save(\n","      dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n","      save_path,\n","    )\n","    print(f\"UnknownFuncRLNet saved to {save_path} at step {self.curr_step}\")\n","\n","  def learn(self):\n","    if self.curr_step % self.sync_every == 0:\n","      self.sync_Q_target()\n","\n","    if self.curr_step % self.save_every == 0:\n","      self.save()\n","\n","    if self.curr_step < self.burnin:\n","      return None, None\n","\n","    if self.curr_step % self.learn_every != 0:\n","      return None, None\n","\n","    # Sample from memory\n","    state, next_state, action, reward, done = self.recall()\n","\n","    # Get TD Estimate\n","    td_est = self.td_estimate(state, action)\n","\n","    # Get TD Target\n","    td_tgt = self.td_target(reward, next_state, done)\n","\n","    # Backpropagate loss through Q_online\n","    loss = self.update_Q_online(td_est, td_tgt)\n","\n","    return (td_est.mean().item(), loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"7kpOloIASW5L","outputId":"3d359f8d-61cc-4ca8-a7ab-3fb984ca750b"},"outputs":[],"source":["#@title Game.... { form-width: \"10%\" }\n","save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n","save_dir.mkdir(parents=True)\n","\n","env = UnknownFuncEnv(epochs = 200, debug = False, use_const = False, lr_task = 0.002)\n","agent = UnknownFuncAgent(state_dim=(1, 1), action_dim=env.action_space.n, save_dir=save_dir)\n","\n","episodes = 10000\n","for e in range(episodes):\n","  print(f'Starting play episode {e}')\n","\n","  state = env.reset()\n","\n","  # Play the game!\n","  while True:\n","    # Run agent on the state\n","    action = agent.act(state)\n","\n","    # Agent performs action\n","    next_state, reward, done, info = env.step(action)\n","\n","    # Remember\n","    agent.cache(state, next_state, action, reward, done)\n","\n","    # Learn\n","    q, loss = agent.learn()\n","\n","    # Update state\n","    state = next_state\n","\n","    # Check if end of game\n","    if done:\n","        break\n","\n","  print(f'Episode {e} completed, reward is {env.total_reward}, exploration rate {agent.exploration_rate}\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPKrqMrnxa0jnWK8icfqUVW","collapsed_sections":[],"name":"lr_rl.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
